{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3f11a-98d8-4341-b4b2-1f07a2dec340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing, Cleaning, and Merging of Stock Returns and Sector Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# File Locations\n",
    "#Input file paths\n",
    "SECTOR_INFO_FILE_PATH = 'Sector Information.xlsm'  # Original sector info\n",
    "STOCK_RETURN_FILE_PATH = 'Stock_Return_Data2023Fwd.xlsx'\n",
    "STOCK_VARREF_FILE_PATH = 'Financial Ratios_Variable_Reference.xlsx'\n",
    "SIC_CODE_FILE_PATH = 'sic_codes_wikipedia.xlsx'\n",
    "\n",
    "#Output file paths\n",
    "SECTOR_INFO_wrds_FILE_PATH = 'sector_information_wrds.xlsx'  # WRDS sector info\n",
    "SECTOR_INFO_UPDATED_FILE_PATH = 'sector_information_updated_with_wrds.xlsx'  # Output file\n",
    "STOCK_DENG_OUT_FILE_PATH = 'Sector_Info_Return_Ratio_Merged.xlsx' # Final Output file\n",
    "\n",
    "\n",
    "# Source Sector Information from Wharton Research Data Service (WRDS)\n",
    "\n",
    "# Flag - Enable Sector Name retrieval from WRDS. Valid credentials required. Best Source for Sector Info\n",
    "EXTRACT_wrds_SECTOR_INFO_FLAG = False\n",
    "\n",
    "if EXTRACT_wrds_SECTOR_INFO_FLAG:\n",
    "    SIC_CODE_FILE_PATH = 'sic_codes_wikipedia.xlsx'\n",
    "    df_sic = pd.read_excel(SIC_CODE_FILE_PATH)\n",
    "    # Connect to WRDS\n",
    "    db = wrds.Connection()\n",
    "    \n",
    "    \n",
    "    # SQL Query to get GVKEY, PERMNO, TICKER, and Sector Name\n",
    "    sql_code=\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        CAST(c.gvkey AS INTEGER) AS \"gvkey_wrds\",\n",
    "        CAST(l.lpermno AS INTEGER) AS \"permno_wrds\",\n",
    "        c.conm AS \"comnam_wrds\",\n",
    "        CASE \n",
    "            WHEN s.ticker = 'N/A' OR s.ticker IS NULL THEN ''  -- Set to empty string if 'N/A' or NULL\n",
    "            ELSE s.ticker \n",
    "        END AS ticker_wrds,\n",
    "        CAST(c.sic AS INTEGER) AS \"sic_code_wrds\",\n",
    "        CASE \n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 100 AND 999 THEN 'Agriculture, Forestry, and Fishing'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 1000 AND 1499 THEN 'Mining'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 1500 AND 1799 THEN 'Construction'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 2000 AND 3999 THEN 'Manufacturing'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 4000 AND 4999 THEN 'Transportation, Communications, Utilities'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 5000 AND 5199 THEN 'Wholesale Trade'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 5200 AND 5999 THEN 'Retail Trade'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 6000 AND 6799 THEN 'Finance, Insurance, and Real Estate'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 7000 AND 8999 THEN 'Services'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 9100 AND 9729 THEN 'Public Administration'\n",
    "            WHEN CAST(c.sic AS INTEGER) BETWEEN 9900 AND 9999 THEN 'Nonclassifiable'\n",
    "            ELSE ''\n",
    "        END AS \"sic_division_wrds\",\n",
    "        CAST(c.gsector AS INTEGER) AS \"sector_code_wrds\",\n",
    "        CASE \n",
    "            WHEN CAST(c.gsector AS INTEGER) = 10 THEN 'Energy'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 15 THEN 'Materials'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 20 THEN 'Industrials'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 25 THEN 'Consumer Discretionary'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 30 THEN 'Consumer Staples'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 35 THEN 'Health Care'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 40 THEN 'Financials'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 45 THEN 'Information Technology'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 50 THEN 'Communication Services'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 55 THEN 'Utilities'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 60 THEN 'Real Estate'\n",
    "            ELSE ''\n",
    "        END AS \"sector_name_wrds\",  \n",
    "        CASE \n",
    "            WHEN CAST(c.gsector AS INTEGER) = 10 THEN 'Energy: Oil, gas, coal, and fuels'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 15 THEN 'Materials: Chemicals, metals, mining, forestry'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 20 THEN 'Industrials: Manufacturing, machinery, transportation'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 25 THEN 'Consumer Discretionary: Automobiles, hotels, media'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 30 THEN 'Consumer Staples: Food, beverages, household goods'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 35 THEN 'Health Care: Pharmaceuticals, biotech, hospitals'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 40 THEN 'Financials: Banks, insurance, investment services'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 45 THEN 'Information Technology: Software, IT services, semiconductors'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 50 THEN 'Communication Services: Media, telecom, entertainment'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 55 THEN 'Utilities: Electric, gas, water utilities'\n",
    "            WHEN CAST(c.gsector AS INTEGER) = 60 THEN 'Real Estate: REITs, property development'\n",
    "            ELSE ''\n",
    "        END AS \"sector_description_wrds\"  \n",
    "    FROM comp.company AS c\n",
    "    JOIN crsp.ccmxpf_lnkhist AS l\n",
    "        ON c.gvkey = l.gvkey\n",
    "        AND l.linktype IN ('LU', 'LC')\n",
    "        AND l.linkprim IN ('P', 'C')\n",
    "    JOIN crsp.msenames AS s\n",
    "        ON l.lpermno = s.permno\n",
    "    ORDER BY gvkey_wrds;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fetch data\n",
    "    df_wrds = db.raw_sql(sql_code)\n",
    "    df_wrds = df_wrds.merge(df_sic, left_on=['sic_code_wrds'], right_on=['sic_code'], how=\"left\")\n",
    "    df_wrds.rename(columns={'sic_industry': 'sic_industry_wrds'}, inplace=True)\n",
    "    df_wrds.drop(columns=['sic_code'], inplace=True)\n",
    "\n",
    "    df_wrds['ticker_wrds'] = df_wrds['ticker_wrds'].str.strip()\n",
    "    df_wrds['ticker_wrds']=df_wrds['ticker_wrds'].replace('#N/A', '').replace('N/A','')\n",
    "    df_wrds['ticker_wrds']=df_wrds['ticker_wrds'].replace('', np.nan)\n",
    "     \n",
    "    df_wrds = df_wrds.dropna(subset=['ticker_wrds'])\n",
    "    \n",
    "    # Save as Excel\n",
    "    df_wrds.to_excel(SECTOR_INFO_wrds_FILE_PATH, index=False, sheet_name='sector data')\n",
    "    \n",
    "    print(f\"Data saved as {SECTOR_INFO_wrds_FILE_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "# Read stock information (renamed to sector_info_orig_df)\n",
    "sector_info_orig_df = pd.read_excel(SECTOR_INFO_FILE_PATH, sheet_name='sector data')\n",
    "\n",
    "# Convert TICKER columns to uppercase and remove spaces to avoid case mismatch\n",
    "sector_info_orig_df['TICKER'] = sector_info_orig_df['TICKER'].str.upper().str.replace(' ', '')\n",
    "\n",
    "# Read sector information from WRDS\n",
    "sector_info_wrds_df = pd.read_excel(SECTOR_INFO_wrds_FILE_PATH)\n",
    "\n",
    "# Drop rows with missing values in 'permno' or 'ticker'\n",
    "sector_info_wrds_df.dropna(subset=['permno_wrds', 'ticker_wrds', 'sector_name_wrds'], inplace=True)\n",
    "\n",
    "# Drop duplicate rows\n",
    "sector_info_wrds_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Merge based on 'permno' to retrieve matching sector name and ticker\n",
    "merged_df = pd.merge(\n",
    "    sector_info_orig_df, \n",
    "    sector_info_wrds_df, \n",
    "    left_on=['permno', 'TICKER'], \n",
    "    right_on=['permno_wrds', 'ticker_wrds'], \n",
    "    how='left', \n",
    "    #suffixes=('', '_wrds')\n",
    ")\n",
    "\n",
    "# Fill missing sector names in original data with sector_name from new_entries\n",
    "merged_df['Sector Name'] = merged_df.apply(\n",
    "    lambda row: row['sector_name_wrds'] if pd.isna(row['Sector Name']) and pd.notna(row['sector_name_wrds']) else row['Sector Name'], axis=1\n",
    " )\n",
    "\n",
    "# Fill missing sector names in original data with sector_name from new_entries\n",
    "merged_df['TICKER'] = merged_df.apply(\n",
    "    lambda row: row['ticker_wrds'] if pd.isna(row['TICKER']) and pd.notna(row['ticker_wrds']) else row['TICKER'], axis=1\n",
    ")\n",
    "merged_df['New'] = 'N' \n",
    "\n",
    "# Identify unique records in WRDS data that are missing in the original data\n",
    "new_entries = sector_info_wrds_df[\n",
    "    ~sector_info_wrds_df['ticker_wrds'].isin(merged_df['TICKER']) |\n",
    "    ~sector_info_wrds_df['permno_wrds'].isin(merged_df['permno'])\n",
    "]\n",
    "\n",
    "new_entries = new_entries.dropna(subset=['permno_wrds', 'ticker_wrds', 'sector_name_wrds'])  # Ensure permno, ticker, and sector_name are non-null\n",
    "new_entries['gvkey'] = new_entries['gvkey_wrds']\n",
    "new_entries['permno'] = new_entries['permno_wrds']\n",
    "new_entries['TICKER'] = new_entries['ticker_wrds']\n",
    "new_entries['Sector Name'] = new_entries['sector_name_wrds']\n",
    "new_entries['New'] = 'Y'\n",
    "\n",
    "# Append the unique new entries to the merged dataframe\n",
    "#new_entries.rename(columns={'ticker': 'ticker_wrd', 'sector_name': 'sector_name_wrd'}, inplace=True)\n",
    "merged_df = pd.concat([merged_df, new_entries], ignore_index=True)\n",
    "\n",
    "merged_df['TICKER']=merged_df['TICKER'].replace('#N/A', '').replace('N/A','')\n",
    "merged_df['TICKER']=merged_df['TICKER'].replace('', np.nan)\n",
    "\n",
    "merged_df['Sector Name']=merged_df['Sector Name'].replace('#N/A', '').replace('N/A','')\n",
    "merged_df['Sector Name']=merged_df['Sector Name'].replace('', np.nan)\n",
    "\n",
    "merged_df['ticker_wrds']=merged_df['ticker_wrds'].replace('#N/A', '').replace('N/A','')\n",
    "merged_df['ticker_wrds']=merged_df['ticker_wrds'].replace('', np.nan)\n",
    "\n",
    "merged_df['sector_name_wrds']=merged_df['sector_name_wrds'].replace('#N/A', '').replace('N/A','')\n",
    "merged_df['sector_name_wrds']=merged_df['sector_name_wrds'].replace('', np.nan)\n",
    "\n",
    "# Drop rows with missing values in 'permno' or 'ticker', 'sector_name_wrds'\n",
    "merged_df.dropna(subset=['permno_wrds', 'ticker_wrds', 'sector_name_wrds'], inplace=True)\n",
    "\n",
    "# Drop duplicate records\n",
    "merged_df.drop_duplicates(subset=['permno_wrds','ticker_wrds'], inplace=True)\n",
    "\n",
    "merged_df = merged_df.astype({col: 'Int64' for col in merged_df.select_dtypes('float').columns})\n",
    "\n",
    "merged_df['PERMNO_TICKER_LABEL'] = merged_df['permno_wrds'].astype(str) + '-' + merged_df['ticker_wrds'].astype(str)\n",
    "\n",
    "print('Ticker symbols cleaned')\n",
    "\n",
    "\n",
    "# Save the updated dataframe to a new Excel file\n",
    "merged_df.to_excel(SECTOR_INFO_UPDATED_FILE_PATH, index=False)\n",
    "\n",
    "\n",
    "print(f\"Updated stock information saved to '{SECTOR_INFO_UPDATED_FILE_PATH}'\")\n",
    "\n",
    "df_cleaned = merged_df\n",
    "\n",
    "\n",
    "# Create Sector Info Lookup Table to Merge with Stock Revenue\n",
    "df_cleaned_lkup = df_cleaned[['PERMNO_TICKER_LABEL', 'ticker_wrds', 'sector_name_wrds']].drop_duplicates().copy()\n",
    "df_cleaned_lkup.rename(columns={'sector_name_wrds': 'Sector_Name'}, inplace=True)\n",
    "\n",
    "# Move 'permno_ticker_lkup' to the first column\n",
    "\n",
    "\n",
    "# Read Stock Return Excel file\n",
    "# filename: Stock_Return_Data2023Fwd.xlsx\n",
    "# Worsksheet: \"data\"\n",
    "# columns: (PERMNO, date, YEAR, MONTH, SIC, Industry, Code, TICKER, COMNAM, \n",
    "#            Price, Index, Past, 1-Month, Ret, Past, 3-Month, Ret, Past, \n",
    "#            6-Month, Ret, Past, 12-Month, Ret, Forward, 1-Month, Ret,\n",
    "#            Forward, 3-Month, Ret, Forward, 6-Month, Ret, PERMNO_DATE_LABEL)\n",
    "\n",
    "STOCK_RETURN_FILE_PATH = 'Stock_Return_Data2023Fwd.xlsx'\n",
    "df_re = pd.read_excel(STOCK_RETURN_FILE_PATH)\n",
    "\n",
    "# Add PERMNO_TICKER_LABEL (PERMNO-TECKER) to Stock Return for merging with Stock Information\n",
    "df_re['TICKER'] = df_re['TICKER'].str.strip()\n",
    "if 'PERMNO_TICKER_LABEL' not in df_re.columns:\n",
    "    df_re.loc[df_re[['PERMNO', 'TICKER']].notna().all(axis=1), 'PERMNO_TICKER_LABEL'] = (\n",
    "        df_re['PERMNO'].astype(str) + '-' + df_re['TICKER'].astype(str)\n",
    "    )\n",
    "\n",
    "\n",
    "# drop unnamed column from stock returns dataset\n",
    "if 'Unnamed: 8' in df_re.columns:\n",
    "    df_re.drop(columns=['Unnamed: 8'], inplace=True)\n",
    "    print(f\"dropped blank unnamed column 'Unnamed: 8'\")\n",
    "else:\n",
    "    print(f\"unnamed column 'Unnamed: 8' already removed\")\n",
    "\n",
    "# Merge df_re with df_cleaned_lkup to add Sector Name only once based on PERMNO_TICKER_LABEL\n",
    "if 'Sector_Name' not in df_re.columns:\n",
    "    df_re = df_re.merge(\n",
    "    df_cleaned_lkup[['PERMNO_TICKER_LABEL', 'Sector_Name']], \n",
    "    on='PERMNO_TICKER_LABEL', \n",
    "    how='left'\n",
    ")\n",
    "print(f'Sector name from [sector_information_updated] added to Stock Returns')\n",
    "\n",
    "columns_to_fill_with_zeros = [\n",
    "    'Past 1-Month Ret', 'Past 3-Month Ret', 'Past 6-Month Ret', 'Past 12-Month Ret',\n",
    "    'Forward 1-Month Ret', 'Forward 3-Month Ret', 'Forward 6-Month Ret'\n",
    "]\n",
    "\n",
    "df_re[columns_to_fill_with_zeros] = (df_re[columns_to_fill_with_zeros]\n",
    "    .apply(pd.to_numeric, errors='coerce')\n",
    "    .astype(float)\n",
    "    .fillna(0.000)\n",
    ")\n",
    "\n",
    "# Read Financial Ratios Data Excel file\n",
    "# filename: Financial Ratios Data 2003_2023x.xlsm\n",
    "# Worsksheet: \"financial_ratios\"\n",
    "# columns: \n",
    "# 'gvkey', 'permno', 'adate', 'qdate', 'public_date', 'TICKER', 'year', 'month', 'PERMNO-YearMonth', 'CAPEI', \n",
    "# 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', 'npm', 'opmbd', 'opmad', 'gpm', 'ptpm', \n",
    "# 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq', 'aftret_invcapx', 'aftret_equity', 'pretret_noa', \n",
    "# 'pretret_earnat', 'GProf', 'equity_invcap', 'debt_invcap', 'totdebt_invcap', 'capital_ratio', \n",
    "# 'int_debt', 'int_totdebt', 'cash_lt', 'invt_act', 'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', \n",
    "# 'curr_debt', 'lt_debt', 'profit_lct', 'ocf_lct', 'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', \n",
    "# 'debt_assets', 'debt_capital', 'de_ratio', 'intcov', 'intcov_ratio', 'cash_ratio', 'quick_ratio', \n",
    "# 'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn', 'rect_turn', 'pay_turn', 'sale_invcap', \n",
    "# 'sale_equity', 'sale_nwc', 'rd_sale', 'adv_sale', 'staff_sale', 'accrual', 'ptb'\n",
    "\n",
    "STOCK_FINANCIAL_RATIOS_FILE_PATH = 'Financial Ratios Data 2003_2023x.xlsm'\n",
    "df_fr = pd.read_excel(STOCK_FINANCIAL_RATIOS_FILE_PATH, skiprows=1)\n",
    "\n",
    "# merge stock returns with financial ratios\n",
    "df_fr['TICKER'] = df_fr['TICKER'].str.strip()\n",
    "# Drop unwanted columns from df_fr\n",
    "\n",
    "for col in ['gvkey', 'permno']:\n",
    "    if col in df_fr.columns:\n",
    "        df_fr.drop(columns=[col], inplace=True)\n",
    "\n",
    "df_re_fr = df_re.merge(df_fr, left_on=['PERMNO_DATE_LABEL', 'TICKER'], right_on=['PERMNO-YearMonth', 'TICKER'], how='left')\n",
    "\n",
    "# Exclude Records with Sector Name in Financials, 'Real Estate', 'Utilities' Per project Instruction\n",
    "excluded_rec_count = df_re['Sector_Name'].isin(['Financials', 'Real Estate', 'Utilities']).sum()\n",
    "print(f\"\\nDropped records in ['Financials', 'Real Estate', 'Utilities']: {excluded_rec_count}\\n\")\n",
    "\n",
    "df_re_fr=df_re_fr[\n",
    "    (~df_re_fr['Sector_Name'].isin(['Financials', 'Real Estate', 'Utilities'])) &  # Exclude sectors\n",
    "    (df_re_fr['TICKER'].notna()) &  # Remove NaN or empty TICKER\n",
    "    (df_re_fr['Sector_Name'].notna())]\n",
    "\n",
    "# Identify TICKER groups where all values in a column are NaN\n",
    "\n",
    "columns_to_interpolate = [\n",
    "    'CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', 'npm',\n",
    "    'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq',\n",
    "    'aftret_invcapx', 'aftret_equity', 'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap',\n",
    "    'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt', 'int_totdebt', 'cash_lt',\n",
    "    'invt_act', 'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt',\n",
    "    'profit_lct', 'ocf_lct', 'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets',\n",
    "    'debt_capital', 'de_ratio', 'intcov', 'intcov_ratio', 'cash_ratio', 'quick_ratio',\n",
    "    'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn', 'rect_turn', 'pay_turn',\n",
    "    'sale_invcap', 'sale_equity', 'sale_nwc', 'rd_sale', 'adv_sale', 'staff_sale', 'accrual', 'ptb'\n",
    "]\n",
    "problematic_tickers = {}\n",
    "\n",
    "for col in columns_to_interpolate:\n",
    "    empty_groups = df_re_fr.groupby('TICKER')[col].apply(lambda x: x.isna().all())\n",
    "    tickers_with_all_nan = empty_groups[empty_groups].index.tolist()\n",
    "    \n",
    "    if tickers_with_all_nan:\n",
    "        problematic_tickers[col] = tickers_with_all_nan\n",
    "\n",
    "print(\"Columns with all-NaN TICKER groups:\")\n",
    "for col, tickers in problematic_tickers.items():\n",
    "    print(f\"{col}: {tickers}\")\n",
    "\n",
    "\n",
    "# ### drop tickers with all NA financial ratios\n",
    "\n",
    "\n",
    "# Define the list of tickers to drop\n",
    "tickers_to_drop = {'AIZ', 'AQU', 'BF', 'CCL', 'DALN', 'DE', 'DHI', 'DISCA', 'DPZ', 'ESV',\n",
    "                   'ET', 'ETE', 'GOOGL', 'HCP', 'IDNX', 'JXJT', 'KBH', 'KBSF', 'KMRT', 'LEN',\n",
    "                   'MDR', 'MXB', 'NOW', 'NVR', 'ODETA', 'ODETB', 'PARA', 'PARAA', 'PCAR',\n",
    "                   'PHM', 'PX', 'SLB', 'TNL', 'TUGC', 'TXT', 'UAA', 'UN', 'V', 'VIAC',\n",
    "                   'VIACA', 'VTR', 'WBR'}\n",
    "\n",
    "# Drop records with tickers in the list\n",
    "df_re_fr = df_re_fr[~df_re_fr['TICKER'].isin(tickers_to_drop)]\n",
    "\n",
    "# Reset index after filtering\n",
    "df_re_fr = df_re_fr.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Reset index to avoid multi-index issues\n",
    "df_re_fr.reset_index(inplace=True)\n",
    "\n",
    "# Convert public_date to datetime and create YEAR column\n",
    "df_re_fr['public_date'] = pd.to_datetime(df_re_fr['public_date'])\n",
    "\n",
    "# Set index properly for sorting\n",
    "df_re_fr = df_re_fr.set_index(['PERMNO_TICKER_LABEL', 'public_date']).sort_index(level=['PERMNO_TICKER_LABEL', 'public_date'])\n",
    "\n",
    "# List of financial ratios to interpolate\n",
    "columns_to_interpolate = [\n",
    "    'CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', 'npm',\n",
    "    'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq',\n",
    "    'aftret_invcapx', 'aftret_equity', 'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap',\n",
    "    'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt', 'int_totdebt', 'cash_lt',\n",
    "    'invt_act', 'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt',\n",
    "    'profit_lct', 'ocf_lct', 'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets',\n",
    "    'debt_capital', 'de_ratio', 'intcov', 'intcov_ratio', 'cash_ratio', 'quick_ratio',\n",
    "    'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn', 'rect_turn', 'pay_turn',\n",
    "    'sale_invcap', 'sale_equity', 'sale_nwc', 'rd_sale', 'adv_sale', 'staff_sale', 'accrual', 'ptb'\n",
    "]\n",
    "\n",
    "# Function to apply hybrid imputation within each TICKER group\n",
    "def impute_financial_ratios(group):\n",
    "    \"\"\"Hybrid Imputation for Financial Metrics\"\"\"\n",
    "    group[columns_to_interpolate] = group[columns_to_interpolate].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Only apply rolling median where at least one non-null value exists\n",
    "    for col in columns_to_interpolate:\n",
    "        if not group[col].isnull().all():  # Check if the column has any non-null values\n",
    "            group[col] = (\n",
    "                group[col]\n",
    "                .fillna(group[col].rolling(3, min_periods=1).median())\n",
    "                .fillna(group[col].rolling(6, min_periods=1).median())\n",
    "                .fillna(group[col].rolling(9, min_periods=1).median())\n",
    "                .fillna(group[col].rolling(12, min_periods=1).median())\n",
    "            )\n",
    "\n",
    "    # Final backfill & forward fill\n",
    "    group[columns_to_interpolate] = group[columns_to_interpolate].bfill().ffill()\n",
    "\n",
    "    return group\n",
    "\n",
    "# Apply per TICKER\n",
    "df_re_fr = df_re_fr.groupby(level='PERMNO_TICKER_LABEL', group_keys=False).apply(impute_financial_ratios)\n",
    "\n",
    "# Final step: Fill remaining missing values using YEAR and TICKER median\n",
    "df_re_fr = df_re_fr.reset_index()\n",
    "df_re_fr[columns_to_interpolate] = df_re_fr.groupby(['PERMNO_TICKER_LABEL'])[columns_to_interpolate].transform(lambda x: x.fillna(x.median(skipna=True)))\n",
    "\n",
    "# Reset index\n",
    "df_re_fr.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# Save merged datset to excel file. Can continue analysis with df_re\n",
    "\n",
    "# Export file for merged Stock Returns and Sector Name dataset\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(STOCK_DENG_OUT_FILE_PATH):\n",
    "    # The file exists\n",
    "    print(f'Deleting File {STOCK_DENG_OUT_FILE_PATH} and resaving')\n",
    "    os.remove(STOCK_DENG_OUT_FILE_PATH) \n",
    "    \n",
    "else:\n",
    "# The file does not exist\n",
    "    print(f'Saving New File to {STOCK_DENG_OUT_FILE_PATH}')\n",
    "\n",
    "df_re_fr.to_excel(STOCK_DENG_OUT_FILE_PATH, index=False)\n",
    "print(f'File Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396b6ce-f2b6-4dd4-925a-30ac0b65d5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
